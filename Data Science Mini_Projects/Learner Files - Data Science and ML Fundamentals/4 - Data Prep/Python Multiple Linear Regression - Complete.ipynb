{"cells":[{"cell_type":"markdown","id":"82113fee","metadata":{"id":"82113fee"},"source":["# Multiple Linear Regression"]},{"cell_type":"markdown","id":"4fd48639","metadata":{"id":"4fd48639"},"source":["In this notebook we will expand our simple linear regression model that we built to predict car prices in the last chapter to include several independent variables in order to produce better predictions."]},{"cell_type":"markdown","id":"e08c665f","metadata":{"id":"e08c665f"},"source":["### Package and Data Loading\n","\n","As before, we will import the required packages and our car price data set."]},{"cell_type":"code","execution_count":null,"id":"06b403ac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1509,"status":"ok","timestamp":1642013444873,"user":{"displayName":"Joseph Yeates","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgPAHtRWS5ii31Agylr2g96Bdfynlb_kGIjZ3P=s64","userId":"05262449329418699827"},"user_tz":480},"id":"06b403ac","outputId":"5fcb07e4-b395-4d75-ca0a-8fd2e215ad2f"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plot\n","import statsmodels.api as stats\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"f4865a4d","metadata":{"id":"f4865a4d"},"outputs":[],"source":["carprice_df = pd.read_csv('CarPrice_Assignment.csv')"]},{"cell_type":"markdown","id":"fdb2c710","metadata":{"id":"fdb2c710"},"source":["### Assessing the Data"]},{"cell_type":"code","execution_count":null,"id":"94e1557d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"elapsed":528,"status":"error","timestamp":1642013617089,"user":{"displayName":"Joseph Yeates","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgPAHtRWS5ii31Agylr2g96Bdfynlb_kGIjZ3P=s64","userId":"05262449329418699827"},"user_tz":480},"id":"94e1557d","outputId":"048bf045-847c-48e7-fd45-8523aaea281b"},"outputs":[],"source":["carprice_df.shape"]},{"cell_type":"code","execution_count":null,"id":"4fa9c004","metadata":{"id":"4fa9c004"},"outputs":[],"source":["carprice_df.head()"]},{"cell_type":"markdown","id":"f8dc38b4","metadata":{"id":"f8dc38b4"},"source":["We can see all the columns and what data type they are using [df.info()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) and how many unique values there are for the categorical types using [df.nunique()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.nunique.html)."]},{"cell_type":"code","execution_count":null,"id":"09e79eb4","metadata":{"id":"09e79eb4"},"outputs":[],"source":["carprice_df.info()"]},{"cell_type":"markdown","id":"3ed9c20d","metadata":{"id":"3ed9c20d"},"source":["Here we are checking the number of unique values in specifically the categorical variables using [df.select_dtypes()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)"]},{"cell_type":"code","execution_count":null,"id":"72e73d4b","metadata":{"id":"72e73d4b"},"outputs":[],"source":["carprice_df.select_dtypes(include='object').nunique()"]},{"cell_type":"markdown","id":"b2c8022f","metadata":{"id":"b2c8022f"},"source":["As we can see, the data contains a mixture of numeric types and categorical (object) types. We will remove the car_ID field from the data as this is only an identifier. For the purposes of this lesson we will also remove CarName from the data as it contains a large number of unique values (How could we extract more useful information from this variable?)."]},{"cell_type":"code","execution_count":null,"id":"da0d6f86","metadata":{"id":"da0d6f86"},"outputs":[],"source":["carprice_df = carprice_df.drop(columns=['car_ID', 'CarName'])"]},{"cell_type":"markdown","id":"d286e93c","metadata":{"id":"d286e93c"},"source":["## Basic Multiple Regression Model"]},{"cell_type":"markdown","id":"f51018b4","metadata":{"id":"f51018b4"},"source":["Before we build our full model using all of the data available to us in the dataset we will first build a straightforward model using four independent variables we think might be relevant to the price to practice fitting the model. The actual fitting of the model is very similar to how we did it previously for the simple linear regression model using statsmodels. We will build it using enginesize from the simple model plus curbweight, peakrpm and citympg."]},{"cell_type":"code","execution_count":null,"id":"ecea0899","metadata":{"id":"ecea0899"},"outputs":[],"source":["Y_basic = carprice_df.price\n","X_basic = stats.add_constant(carprice_df[['enginesize', 'curbweight', 'peakrpm', 'citympg']])"]},{"cell_type":"markdown","id":"54c925bb","metadata":{"id":"54c925bb"},"source":["The only difference to the previous chapter is that we add a constant column to our dataframe of multiple independent variables instead of to a single independent variable. The fitting process is also exactly the same."]},{"cell_type":"code","execution_count":null,"id":"b96ecac4","metadata":{"id":"b96ecac4"},"outputs":[],"source":["model_basic = stats.OLS(Y_basic, X_basic)\n","results_basic = model_basic.fit()"]},{"cell_type":"markdown","id":"9f3a5bec","metadata":{"id":"9f3a5bec"},"source":["We can see our results and the parameters for each of the independent variables using the .summary() attribute again."]},{"cell_type":"code","execution_count":null,"id":"10506c50","metadata":{"id":"10506c50"},"outputs":[],"source":["print(results_basic.summary())"]},{"cell_type":"markdown","id":"27d3575c","metadata":{"id":"27d3575c"},"source":["With these parameter values we can construct our model:\n","\n","$\\textrm{price}=116*\\textrm{enginesize}+ 5.7*\\textrm{curbweight}+2.7*\\textrm{peakrpm}+10.8*\\textrm{citympg}-30220$"]},{"cell_type":"markdown","id":"31956d89","metadata":{"id":"31956d89"},"source":["## Full Multiple Regression Model"]},{"cell_type":"markdown","id":"a5627b9b","metadata":{"id":"a5627b9b"},"source":["We can now look at building our final model using the full range of features available to us. Before we build the model we need to prepare the data and reduce the number of independent variables we have."]},{"cell_type":"markdown","id":"7befedf5","metadata":{"id":"7befedf5"},"source":["We can look at the correlations between different numerical variables in a handy way using a correlation matrix - this allows us to see the correlation between all pairs of variables at once. We can then remove some of the independent variables that are highly correlated and would cause problems with the algorithm due to multicollinearity. We can create this correlation matrix using the [df.corr()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) method. We add a red/blue heatmap to better see where the extreme correlations are."]},{"cell_type":"code","execution_count":null,"id":"1e641dab","metadata":{"id":"1e641dab"},"outputs":[],"source":["carprice_df.select_dtypes(exclude='object').corr().style.background_gradient(cmap='coolwarm')"]},{"cell_type":"markdown","id":"1fa41242","metadata":{"id":"1fa41242"},"source":["We can see that highwaympg and citympg are highly correlated with a correlation coefficient of 0.97. Removing highwympg will get rid of this correlation and help reduce the complexity of our model. We also choose to remove carlength and carwidth to remove some more high correlations. "]},{"cell_type":"code","execution_count":null,"id":"c5ea1d43","metadata":{"id":"c5ea1d43"},"outputs":[],"source":["carprice_df = carprice_df.drop(columns=['carlength', 'carwidth', 'highwaympg'])"]},{"cell_type":"markdown","id":"878eabcf","metadata":{"id":"878eabcf"},"source":["#### One Hot Encoding\n","\n","We can also use the categorical data by one-hot-encoding the it. This is where we make each category in a categorical variable its own independent variable which has a binary 1/0 value. We use the function [pd.get_dummies()](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to do this for the categorical values and then join them back to the numerical variables using [pd.concat()](https://pandas.pydata.org/docs/reference/api/pandas.concat.html). We drop the first value to prevent multicollinearity. "]},{"cell_type":"code","execution_count":null,"id":"b11ef531","metadata":{"id":"b11ef531"},"outputs":[],"source":["dummy = pd.get_dummies(carprice_df.select_dtypes(include='object'), drop_first=True)"]},{"cell_type":"code","execution_count":null,"id":"5ac0cb80","metadata":{"id":"5ac0cb80"},"outputs":[],"source":["carprice_df = pd.concat([carprice_df.select_dtypes(exclude='object'), dummy], axis=1)"]},{"cell_type":"markdown","id":"19fd576c","metadata":{"id":"19fd576c"},"source":["We can repeat the above process where we remove highly correlated variables, now including the one hot encoded features."]},{"cell_type":"code","execution_count":null,"id":"149d5e75","metadata":{"id":"149d5e75"},"outputs":[],"source":["carprice_df.corr().style.background_gradient(cmap='coolwarm')"]},{"cell_type":"code","execution_count":null,"id":"8d2f1731","metadata":{"id":"8d2f1731"},"outputs":[],"source":["carprice_df = carprice_df.drop(columns=['compressionratio', 'drivewheel_fwd', 'enginetype_rotor', 'fuelsystem_4bbl', 'fuelsystem_idi'])"]},{"cell_type":"code","execution_count":null,"id":"0512fb13","metadata":{"id":"0512fb13"},"outputs":[],"source":["carprice_df.shape"]},{"cell_type":"markdown","id":"26e45f19","metadata":{"id":"26e45f19"},"source":["We now have 35 independent variables (plus our target variable price) to use in our regression model."]},{"cell_type":"markdown","id":"363aebd7","metadata":{"id":"363aebd7"},"source":["### Test/Train Split\n","\n","As in the previous notebook, we will split our data with 70% into the training set and the remaining 30% into the test set. "]},{"cell_type":"code","execution_count":null,"id":"b3c0bac1","metadata":{"id":"b3c0bac1"},"outputs":[],"source":["train_df=carprice_df.sample(frac=0.7, random_state=99) #random state is a seed value\n","test_df=carprice_df.drop(train_df.index)"]},{"cell_type":"code","execution_count":null,"id":"9f5fbde2","metadata":{"id":"9f5fbde2"},"outputs":[],"source":["train_df.shape"]},{"cell_type":"code","execution_count":null,"id":"9739549d","metadata":{"id":"9739549d"},"outputs":[],"source":["test_df.shape"]},{"cell_type":"markdown","id":"e9c41924","metadata":{"id":"e9c41924"},"source":["### Fitting the Linear Regression Model"]},{"cell_type":"markdown","id":"8b5b5e9f","metadata":{"id":"8b5b5e9f"},"source":["We once again use statsmodels to fit our linear regression model. We do this in the same way as the previous notebook except now our X_train contains all of our independent variables (plus the constant column)."]},{"cell_type":"code","execution_count":null,"id":"f7cc806d","metadata":{"id":"f7cc806d"},"outputs":[],"source":["Y_train = train_df.price\n","X_train = stats.add_constant(train_df.drop(columns=['price']))"]},{"cell_type":"code","execution_count":null,"id":"bb30ebfb","metadata":{"id":"bb30ebfb"},"outputs":[],"source":["model_carprice = stats.OLS(Y_train, X_train)\n","results_carprice = model_carprice.fit()"]},{"cell_type":"code","execution_count":null,"id":"56f28f91","metadata":{"id":"56f28f91"},"outputs":[],"source":["print(results_carprice.summary())"]},{"cell_type":"markdown","id":"68c683f9","metadata":{"id":"68c683f9"},"source":["For example here we can see that enginelocation_rear has a coefficient of 7389 so when everything else is constant, a car with the engine in the rear we predict will cost an extra \\\\$7389 than a car with the engine in the front on average. Also we predict that for every unit of weight heavier a car is, the car will cost an extra $3.3. In this state we cannot compare the coefficients to one another as they all have different units - it makes no sense to compare pounds in curbweight with rpm in peakrpm! "]},{"cell_type":"markdown","id":"f90b6f2c","metadata":{"id":"f90b6f2c"},"source":["Our sum of square residuals is then:"]},{"cell_type":"code","execution_count":null,"id":"23d39bae","metadata":{"id":"23d39bae","outputId":"4d3fde8f-fa32-49ba-a7c1-cd8da20b4283"},"outputs":[{"name":"stdout","output_type":"stream","text":["The sum of square residuals is 579728377.8\n"]}],"source":["print('The sum of square residuals is {:.1f}'.format(results_carprice.ssr))"]},{"cell_type":"markdown","id":"b3249f33","metadata":{"id":"b3249f33"},"source":["In our simple linear regression in the last notebook we found a value of 2.3 billion for the SSE whereas here our value is 0.5 billion. As we are training our model on the same number of datapoints, we can see that the multiple linear regression is producing a smaller total error.\n","\n","We can also use our test set to compare our predictions with the observed values."]},{"cell_type":"code","execution_count":null,"id":"9a7de989","metadata":{"id":"9a7de989"},"outputs":[],"source":["Y_test = test_df.price\n","test_df = stats.add_constant(test_df)\n","X_test = test_df[X_train.columns]"]},{"cell_type":"code","execution_count":null,"id":"23f45378","metadata":{"id":"23f45378"},"outputs":[],"source":["test_predictions = results_carprice.predict(X_test)"]},{"cell_type":"code","execution_count":null,"id":"c5a650f9","metadata":{"id":"c5a650f9"},"outputs":[],"source":["plot.scatter(test_predictions, Y_test)\n","plot.plot([5000, 50000], [5000, 50000], c='k', ls='--')\n","plot.xlabel('Predicted Price [$]')\n","plot.ylabel('Observed Price [$]')\n","plot.show()"]},{"cell_type":"markdown","id":"49a37a14","metadata":{"id":"49a37a14"},"source":["Plotting our predicted prices against the observed values again we can see that the points are much tighter to the diagonal line that previously for the simple linear regression model. We will explore the coefficients that we calculate in our model and the metrics used to evaluate them more in the next chapter."]},{"cell_type":"markdown","id":"f04b7e8a","metadata":{"id":"f04b7e8a"},"source":["## Scikit-Learn\n","\n","Again we can repeat this exercise using Scikit-Learn"]},{"cell_type":"code","execution_count":null,"id":"8262a580","metadata":{"id":"8262a580"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","id":"f24100ef","metadata":{"id":"f24100ef"},"source":["### Test/Train Split"]},{"cell_type":"markdown","id":"4da5df1e","metadata":{"id":"4da5df1e"},"source":["When we have multiple independent variables, we do not need to reshape our X array as it is already a 2D array. We can therefore insert our training independent variables straight into the fitting method."]},{"cell_type":"code","execution_count":null,"id":"90a0b423","metadata":{"id":"90a0b423"},"outputs":[],"source":["Y = carprice_df.price\n","X = carprice_df.drop(columns=['price'])"]},{"cell_type":"code","execution_count":null,"id":"8549c968","metadata":{"id":"8549c968"},"outputs":[],"source":["sk_X_train, sk_X_test, sk_Y_train, sk_Y_test = train_test_split(X, Y, test_size=0.3, random_state=99)"]},{"cell_type":"code","execution_count":null,"id":"51345634","metadata":{"id":"51345634"},"outputs":[],"source":["regressor = LinearRegression()  \n","regressor.fit(sk_X_train, sk_Y_train)"]},{"cell_type":"code","execution_count":null,"id":"41e4fa65","metadata":{"id":"41e4fa65"},"outputs":[],"source":["sk_intercept_carprice = regressor.intercept_\n","sk_engsize_coeffs = regressor.coef_\n","sk_ssr_carprice = np.sum((sk_Y_train-regressor.predict(sk_X_train))**2)"]},{"cell_type":"markdown","id":"e97f9af5","metadata":{"id":"e97f9af5"},"source":["Unlike the statsmodels version, the LinearRegression class does not supply a convenient summary of the best fit coefficients however the coefficients are ordered in the same order as the columns are in our X array. We can combine the column names and coefficient values in a pandas Series to better read the values."]},{"cell_type":"code","execution_count":null,"id":"be98ac49","metadata":{"id":"be98ac49"},"outputs":[],"source":["pd.Series(sk_engsize_coeffs, index=sk_X_train.columns)"]},{"cell_type":"code","execution_count":null,"id":"ae34397d","metadata":{"id":"ae34397d"},"outputs":[],"source":["print('The intercept value is {:.1f}'.format(sk_intercept_carprice))\n","print('The sum of square residuals is {:.1f}'.format(sk_ssr_carprice))"]}],"metadata":{"colab":{"name":"Multiple Linear Regression - Complete.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}
